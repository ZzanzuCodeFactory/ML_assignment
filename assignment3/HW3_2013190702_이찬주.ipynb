{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 3 \n",
    "\n",
    "#### Machine Learning in Korea University\n",
    "#### COSE362, Fall 2018\n",
    "#### Due : 12/18 (THU) 11:59 PM\n",
    "\n",
    "#### In this assignment, you will implement and train a Recommender System. Also, you will learn how to utilize scikit-learn to analyze data using clustering.\n",
    "* Implemented using Anaconda 5.3 with python 3.7. Please use <b>python 3</b>\n",
    "* Use given dataset. Please do not change data split.\n",
    "* Use numpy, scikit-learn, and matplotlib library\n",
    "* You don't have to use all imported packages below. (some are optional). <br>\n",
    "* <b>*DO NOT MODIFY OTHER PARTS OF CODES EXCEPT \"Your Code Here\"*</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Recommender System\n",
    "\n",
    "In the problem 1, you will implement a simple recommender system.\n",
    "* See your lecture note (Lecture 15. Recommender Systems - Collaborative filtering). \n",
    "* Do not implement matrix factorization and Do not import any other packages and libraries. <b>You should use only numpy</b>.\n",
    "* Analyze train and validation error. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset description : MovieLens Dataset\n",
    "\n",
    "\n",
    "Here are brief descriptions of the data.\n",
    "\n",
    "><b>u.data</b>     -- The full u data set, 100000 ratings by 943 users on 1682 items.\n",
    "              Each user has rated at least 20 movies.  Users and items are\n",
    "              numbered consecutively from 1.  The data is randomly\n",
    "              ordered. This is a tab separated list of \n",
    "\t         user id | item id | rating | timestamp. \n",
    "              The time stamps are unix seconds since 1/1/1970 UTC\n",
    "\n",
    "><b>u1.base</b> -- Subset of u.data. You should use u1.base at training time. <br>\n",
    "><b>u1.test</b> -- Subset of u.data. You should use u1.test at testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('./data/u.info') as f:\n",
    "    info = f.readlines()\n",
    "    num_users = int(info[0].split()[0])\n",
    "    num_movies = int(info[1].split()[0])\n",
    "\n",
    "def read_data(file_name):\n",
    "    with open('./data/' + file_name) as f:\n",
    "        dataset = []\n",
    "        cnt = 0\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line: break\n",
    "            d = list(map(int, line.split()[:3]))\n",
    "            d[0] -= 1\n",
    "            d[1] -= 1\n",
    "            dataset.append(d)\n",
    "    return dataset\n",
    "\n",
    "###########################################\n",
    "\n",
    "train_dataset = read_data('u1.base')\n",
    "valid_dataset = read_data('u1.test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - (1) Design your recommender system model.\n",
    "\n",
    "##### Instructions <br>\n",
    "* <b>\\__init\\__</b> : configure model. <b>DO NOT MODIFY</b>.\n",
    "* <b>compute_loss</b> : compute loss between user i and movie j.\n",
    "* <b>update</b> : update the parameter of user i and movie j with gradient descent.\n",
    "* <b>run_epoch</b> : train one iteration of collaborative filtering. <b>If \"trainable=False\", this function doesn't update parameters.</b> <br>\n",
    "<t>The variable \"trainable\" is false when the model is on validation or test.\n",
    "    \n",
    "\n",
    "<br>* <b>loss</b> --> see your lecture note.\n",
    "<br>* <b>rmse</b> indicates Root Mean Square Error that is widely used for rating prediction evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RecommenderSystem():\n",
    "    def __init__(self, num_users, num_movies, user_size, movie_size, learning_rate, reg_coef):\n",
    "        self.user_mat = np.random.normal(0, 1, (num_users, user_size))\n",
    "        self.movie_mat = np.random.normal(0, 1, (num_movies, movie_size))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_coef = reg_coef\n",
    "        self.loss = 0.0\n",
    "    \n",
    "    def compute_loss(self, i, j, rating):\n",
    "        # Your Code Here\n",
    "        \n",
    "        # End Your Code\n",
    "        return target, loss\n",
    "        \n",
    "    def update(self, target, i, j, rating):\n",
    "        # Your Code Here\n",
    "        \n",
    "        # End Your Code\n",
    "        ##\n",
    "\n",
    "    def run_epoch(self, dataset, trainable=False):\n",
    "        loss_sum = rmse_sum = target_sum = 0\n",
    "        np.random.shuffle(dataset)\n",
    "        for s_idx, sample in enumerate(dataset):\n",
    "        # Your Code Here\n",
    "            \n",
    "        # End Your Code\n",
    "        return loss_sum / len(dataset), rmse\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(config):\n",
    "    #####\n",
    "    # optimal : (int) the epoch where validation loss is minimum\n",
    "    # eps : (list) a list of training epochs\n",
    "    # loss_tr : (list) a list of training losses\n",
    "    # loss_va : (list) a list of validation losses\n",
    "    # rmse_tr : (list) a list of training rmse(root mean square error)\n",
    "    # rmse_va : (list) a list of validation rmse(root mean square error)\n",
    "    \n",
    "    model = RecommenderSystem(num_users, num_movies,\n",
    "                            config['user_size'],\n",
    "                             config['movie_size'],\n",
    "                             config['learning_rate'],\n",
    "                             config['reg_coef'])\n",
    "    \n",
    "    min_loss = optimal = 99999\n",
    "    eps, loss_tr, loss_va, rmse_tr, rmse_va  = [], [], [], [], []\n",
    "    for epoch in range(config['max_epoch']):\n",
    "        # ls_tr : mean of total losses in an epoch\n",
    "        # e_tr : mean of total root mean square errors in an epoch\n",
    "        ls_tr, e_tr = model.run_epoch(train_dataset, trainable=True)\n",
    "        ls_va, e_va = model.run_epoch(valid_dataset, trainable=False)\n",
    "        \n",
    "        # Your Code Here\n",
    "        \n",
    "        # End Your Code\n",
    "    return optimal, eps, loss_tr, loss_va, rmse_tr, rmse_va, model\n",
    "\n",
    "###################################################################\n",
    "\n",
    "config = {'user_size': 10,\n",
    "         'movie_size': 10,\n",
    "         'learning_rate': 0.01,\n",
    "         'reg_coef': 0.001,\n",
    "         'max_epoch': 50,\n",
    "         'eval_step': 5}\n",
    "\n",
    "optimal, eps, loss_tr, loss_va, rmse_tr, rmse_va, model = main(config)\n",
    "print (\"\\[Exp have been finished !]\\nOptimal : {}, Train loss : {:2.3f}, Valid loss : {:2.3f}, RMSE : {:3.2f}\"\n",
    "       .format(optimal, loss_tr[optimal], loss_va[optimal], rmse_va[optimal]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - (2) Plot the training and validation loss against epochs and Analyze.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your train error and validation error by number of iterations.\n",
    "plt.plot(eps, loss_tr, eps, loss_va, 'r-')\n",
    "plt.title(\"error graph\")\n",
    "plt.legend([\"Training\", \"Validation\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyze the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write description here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering\n",
    "\n",
    "In the problem 2, you would learn how to analyze data with unsupervised learning algorithm.\n",
    "* Implement <b>k-means clustering</b> algorithm using <b>scikit-learn packages</b>. <br>\n",
    "* Visualize your result and analyze. <br>\n",
    "* Implement <b>PCA(principle component analysis)</b> and visualize your data onto 2-dimensional domain, and visualize your data by class-labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import cluster\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "n_samples = 3000\n",
    "random_state = 1182\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - (1) Implement K-means clustering algorithm and visualize data with class labels.\n",
    "For given data, find the best number of clusters (each cluster is well-divided). <br>\n",
    "Visualize your results using <b>scatter plot</b>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn.cluster.KMeans for k-means clustering\n",
    "# Use plt.scatter for visualization\n",
    "\n",
    "# Your Code Here\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - (2) Implement PCA and visualize data with class labels.\n",
    "Conduct K-means clustering on given data. <br>\n",
    "Implement <b>PCA(principle component analysis)</b> to convert high-dimensional vectors into 2-dimensional vectors. <br>\n",
    "Compare plots by K-means result and class labels by visualization. <br>\n",
    "* Dataset : Handwritten digit dataset (Class : digit, Data : digit image)\n",
    "* Visualize <b>two scatter plots</b>. (One for class label and one for k-means clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn.decomposition.PCA\n",
    "\n",
    "digits = load_digits()\n",
    "data = scale(digits.data)\n",
    "labels = digits.target\n",
    "\n",
    "# Your Code Here\n",
    "\n",
    "# End Your Code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
