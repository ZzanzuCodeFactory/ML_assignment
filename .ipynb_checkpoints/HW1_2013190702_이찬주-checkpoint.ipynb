{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #1 \n",
    "\n",
    "#### Machine Learning in Korea University\n",
    "#### COSE362, Fall 2018 (Prof. Jaewoo Kang)\n",
    "#### Due : 11/6 (TUE) 11:59 PM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this assignment, you will learn model selection process among various hyperparameters.\n",
    "* Implementation detail: Anaconda 5.3 with python 3.7\n",
    "* Use given dataset. Please do not change training / validation / test split.\n",
    "* Use numpy, scikit-learn, and matplotlib library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression with Feature Selection\n",
    "In this example we will conduct featrue selection process in linear regression model. <br>\n",
    "You will use data in 'LinReg' directory for this example. <br>\n",
    "Please perform the following steps. \n",
    "> 0. Preprocess: Change given dataset into input array for scikit-learn model.\n",
    "> 1. Feture selection : perform greedy feature selection.\n",
    "> 2. Plot: plot validation and train error against number of feature.\n",
    "> 3. Model selection and evaluation: Select best model and perform evaluation on test dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-0. Preprocess\n",
    "Load dataset and process it into appropriate array form.\n",
    "* Example <br>\n",
    "> For linear regression problem, the datasets are described onto 'dev_sample.npy', 'dev_label.npy', 'test_sample.npy', 'test_label.npy' in 'LinReg' folder. <br>\n",
    "> Load these datasets onto <b>X_dev, y_dev, X_test, y_test</b>. <br>\n",
    "> You may need to use numpy.load function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113400\n",
      "900\n",
      "12600\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Load linear regression datasets\n",
    "# Your code here\n",
    "X_dev = np.load('./LinReg/dev_sample.npy')\n",
    "y_dev = np.load('./LinReg/dev_label.npy')\n",
    "X_test = np.load('./LinReg/test_sample.npy')\n",
    "y_test = np.load('./LinReg/test_label.npy')\n",
    "# End your code\n",
    "\n",
    "print(X_dev.size)\n",
    "print(y_dev.size)\n",
    "print(X_test.size)\n",
    "print(y_test.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Feature selection\n",
    "Build linear regression models with different number of features. (1 ~ 100)<br>\n",
    "Please use <b>cross validation</b>, <b>greedy approach</b> for feature selection until choose optimal number of features. <br> \n",
    "\n",
    "* For cross validaton, you need to split your development set into 5-fold. This is implemented into class <b>cv</b>.\n",
    "* Feature selection example : Input with 10 features\n",
    "> Call 10 features as #1, #2, #3, ..., #10 <br>\n",
    "> First build 10 models with only one feature. \n",
    "> Compare model with #1, model with #2, ... , model with #10 <br>\n",
    "> Choose feature of the best model. (for example, #1 is the best) <br>\n",
    "> Build model with 2 features. (#1, #2), (#1, #3), ..., (#1, #10). <br>\n",
    "> Then, add feature with the best performance. <br>\n",
    "> And so on...\n",
    "\n",
    "<b>For the next step, please save validation and train error of the best model for each number of selected features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define linear regression function\n",
    "# You may use sklearn.linear_model.LinearRegression\n",
    "# Your code here\n",
    "model = LinearRegression()\n",
    "# End your code\n",
    "\n",
    "# Basic settings. DO NOT MODIFY\n",
    "selected_feature = []\n",
    "sel_num = 100\n",
    "valid_split = 1/5\n",
    "cv = ShuffleSplit(n_splits=5, test_size=valid_split, random_state=0)\n",
    "\n",
    "selected_train_error = []\n",
    "selected_valid_error = []\n",
    "\n",
    "# for train_index, test_index in cv.split(X_dev) :\n",
    "#     print('test X:\\n', train_index, test_index)\n",
    "\n",
    "# For greedy selection\n",
    "for sel in range(sel_num) :\n",
    "    min_train_error = +1000\n",
    "    min_valid_error = +1000\n",
    "    min_feature = 0\n",
    "    \n",
    "    # For each feature\n",
    "    for i in range(X_dev.shape[1]) : # zz :126\n",
    "        train_error_ith = []\n",
    "        valid_error_ith = []\n",
    "        \n",
    "        # Select feature greedy\n",
    "        # Hint : There should be no duplicated feature in selected_feature\n",
    "        # Your code here\n",
    "        if i in selected_feature:\n",
    "            continue\n",
    "        else:\n",
    "            X_dev_fs = X_dev[:, [*selected_feature, i]]\n",
    "        # End your code\n",
    "        \n",
    "        # For cross validation\n",
    "        for train_index, test_index in cv.split(X_dev) :\n",
    "            X_train, X_valid = X_dev_fs[train_index], X_dev_fs[test_index]\n",
    "            y_train, y_valid = y_dev[train_index], y_dev[test_index]\n",
    "        \n",
    "            # Derive training error, validation error\n",
    "            # You may use sklearn.metrics.mean_squared_error, model.fit(), model.predict()\n",
    "            # Your code here\n",
    "            fitted_model = model.fit(X_train, y_train)\n",
    "\n",
    "            prediction_train = fitted_model.predict(X_train)\n",
    "            prediction_valid = fitted_model.predict(X_valid)\n",
    "\n",
    "            train_error_ith.append(mean_squared_error(y_train, prediction_train))\n",
    "            valid_error_ith.append(mean_squared_error(y_valid, prediction_valid))\n",
    "            # End your code\n",
    "            \n",
    "        # Select best performance feature set on each features\n",
    "        # You should choose the feature which has minimum mean cross validation error\n",
    "        # Your code here\n",
    "        train_error_average = np.mean(train_error_ith)\n",
    "        valid_error_average = np.mean(valid_error_ith)\n",
    "        \n",
    "        if train_error_average <= min_train_error :\n",
    "            min_train_error = train_error_average \n",
    "        if valid_error_average <= min_valid_error :\n",
    "            min_valid_error = valid_error_average \n",
    "        if valid_error_average <= min_valid_error :\n",
    "            min_feature = i\n",
    "        # End your code\n",
    "    print('='*50)\n",
    "    print(\"# of selected feature(s) : {}\".format(sel+1))\n",
    "    print(\"Selected feature of this iteration : {}\".format(min_feature))\n",
    "    selected_feature.append(min_feature)\n",
    "    selected_train_error.append(min_train_error)\n",
    "    selected_valid_error.append(min_valid_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Plot error\n",
    "Plot train and validation error against number of features.<br>\n",
    "After plotting, <b>analyze the result graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train error plot\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(np.arange(1,sel_num+1), selected_train_error)\n",
    "plt.title('Training error')\n",
    "plt.legend(['error'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation error plot\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(np.arange(1,sel_num+1), selected_valid_error)\n",
    "plt.title('Validation error')\n",
    "plt.legend(['error'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze\n",
    "Write explanation of graph below. <br>\n",
    "Analyze the folloing points.\n",
    "* Trend of each error against number of features\n",
    "* Meaning of gap between vlidation error and train error\n",
    "* Meaning of each region in graph\n",
    "* Others..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature 수를 약 30개까지 늘릴 경우에는 training error와 validation error가 동시에 감소하지만, 이후부터는 training error는 계속 감소하는반면 validation error는 다시 증가하는 형태를 보인다. feature의 수가 약 20개 a미만의 경우 training error와 validation error가 모두 높은 high bias상태를 의미하고(underfit 되어있음), 60개 이상의 경우에는 high varaince를 의미한다(overfitting 발생). high bias의 경우 feature의 개수를 늘려 해결할 수 있고, high variance의 경우 feature의 개수를 다시 줄이거나 @@@하여 해결할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Model selection and evaluation\n",
    "Select the best model and perform a test on test dataset.<br>\n",
    "Print the <b>performance on test set</b> with <b>features of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select optimal feature set corresponding the minimum cross validation error\n",
    "# Your code here\n",
    "min_valid_error = +1000\n",
    "for i, j in enumerate(selected_valid_error) :\n",
    "    if j < min_valid_error :\n",
    "        min_valid_error = j\n",
    "        best_feature_num = i\n",
    "print('best number of features is : ',best_feature_num)\n",
    "\n",
    "X_dev_fs = X_dev[:, selected_feature[:best_feature_num]]\n",
    "# End your code\n",
    "\n",
    "# Basic settings. DO NOT MODIFY\n",
    "min_train_error = 1000\n",
    "min_valid_error = 1000\n",
    "optimal_param = np.array([])\n",
    "\n",
    "for train_index, test_index in cv.split(X_dev) :\n",
    "    X_train, X_valid = X_dev_fs[train_index], X_dev_fs[test_index]\n",
    "    y_train, y_valid = y_dev[train_index], y_dev[test_index]\n",
    "    print('X_train is ', len(X_train),  X_train)\n",
    "    \n",
    "    \n",
    "    # Derive training error, validation error for each fold\n",
    "    # For each fold, you need to compare error with previous minimum error.\n",
    "    # Your code here\n",
    "    fitted_model = model.fit(X_train, y_train)\n",
    "    prediction_train = fitted_model.predict(X_train)\n",
    "    prediction_valid = fitted_model.predict(X_valid)\n",
    "    \n",
    "    mse_train = mean_squared_error(y_train, prediction_train)\n",
    "    mse_valid = mean_squared_error(y_valid, prediction_valid)\n",
    "    \n",
    "    if mse_train < min_train_error :\n",
    "        min_train_error = mse_train\n",
    "    if mse_valid < min_valid_error :\n",
    "        min_valid_error = mse_valid\n",
    "    # End your code\n",
    "\n",
    "# Find the best model on each fold\n",
    "# Derive test error with best performance model\n",
    "# Your code here\n",
    "for test_index in selected_feature[:best_feature_num] :\n",
    "    x_test = X_test[test_index]\n",
    "    y_test_ = y_test[test_index]\n",
    "    \n",
    "    fitted_model = model.fit(x_test, y_test_)\n",
    "    prediction_test = fitted_model.predict(x_test)\n",
    "    test_error = mean_squared_error(y_test_, prediction_test)\n",
    "    \n",
    "# End your code\n",
    "\n",
    "# Drop features of final model\n",
    "print(\"Results\")\n",
    "print(\"# of selected features : {}\".format(len(selected_feature)))\n",
    "print(\"Selected features : \")\n",
    "print(selected_feature)\n",
    "\n",
    "# Drop test error and accuracy\n",
    "print(\"Training error : {}\".format(min_train_error))\n",
    "print(\"Validation error : {}\".format(min_valid_error))\n",
    "print(\"Test error : {}\".format(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression with Regularizer\n",
    "\n",
    "In this example you will explore the effect of regularization parameter.<br>\n",
    "You will use <b>'Heart Disease Dataset'</b> in <b>'LogReg'</b> for this example. <br>\n",
    "\n",
    "The goal is to predict the presence of heart disease given attributes of a patient.<br>\n",
    "The presence is integer valued from 0 (no presence) to 4, but you have to only distingush presensence (values 1,2,3,4) from absence (value 0). <br>\n",
    "Each attribute is described below. <br>\n",
    "\n",
    "> 1. age : age in years <br>\n",
    "> 2. sex : sex (1 = male; 0 = female) <br>\n",
    "> 3. cp : chest pain type <br>\n",
    "-- Value 1: typical angina <br>\n",
    "-- Value 2: atypical angina <br>\n",
    "-- Value 3: non-anginal pain <br>\n",
    "-- Value 4: asymptomatic  <br>\n",
    "> 4. trestbps : resting blood pressure (in mm Hg on admission to the hospital)  <br>\n",
    "> 5. chol : serum cholestoral in mg/dl  <br>\n",
    "> 6. fbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) <br>\n",
    "> 7. restecg  : resting electrocardiographic results <br>\n",
    "-- Value 0: normal <br>\n",
    "-- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) <br>\n",
    "-- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria <br>\n",
    "> 8. thalach : maximum heart rate achieved <br>\n",
    "> 9. exang : exercise induced angina (1 = yes; 0 = no) <br>\n",
    "> 10. oldpeak : ST depression induced by exercise relative to rest <br>\n",
    "> 11. slope : the slope of the peak exercise ST segment <br>\n",
    "-- Value 1: upsloping <br>\n",
    "-- Value 2: flat <br>\n",
    "-- Value 3: downsloping  <br>\n",
    "> 12. ca : number of major vessels (0-3) colored by flourosopy  <br>\n",
    "> 13. thal : 3 = normal; 6 = fixed defect; 7 = reversable defect  <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-0. Preprocess\n",
    "\n",
    "Firstly, read training, validation and test datasets respectively. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_type):\n",
    "    f = open('./LogReg/' + data_type + '.data', 'r')\n",
    "\n",
    "    X, Y = [],[]\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line: break\n",
    "        spl = line.split(',')\n",
    "        x = spl[:-1]\n",
    "        y = int(spl[-1])\n",
    "        \n",
    "        X.append(list(map(float, x)))\n",
    "        \n",
    "        # Define the variable 'binary_label'.\n",
    "        # Note that labels must be 1 or 0.\n",
    "        # Your code here\n",
    "        binary_label = 1 if (y!=0) else 0\n",
    "        \n",
    "        Y.append(binary_label)  # blank\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "X_tr, Y_tr = read_data('train')\n",
    "X_va, Y_va = read_data('valid')\n",
    "X_te, Y_te = read_data('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and Converting to one-hot vector\n",
    "\n",
    "Data preprocessing takes several steps after loading data. <br>\n",
    "1. <b>Normailze</b> numerical values. Normalization is defined as <b><i>normalized_value</i> = (value - mean) / std</b>. <br>\n",
    "   You should calculate mean and standard deviation (std) on <b> train data </b> and normalize train, valid and test data.\n",
    "2. For categorical attributes, <b>build dictionaries</b> of each attribute and convert the categorical values to <b>one-hot vectors</b>. <br>\n",
    "3. Concatenate all the obtained values. <br>\n",
    "\n",
    "If you have done correctly, you will get results that are same format as below: \n",
    "* <b>before</b> : [63.0, 1.0, 1.0, 145.0, 233.0, 1.0, 2.0, 150.0, 0.0, 2.3, 3.0, 0.0, 6.0]\n",
    "* <b>after</b> : [0.11099784710934087, 0, 1, 1, 0, 0, 0, 0.035386000081823056, -0.005256085700922788, 0, 1, 0, 0, 1, 0.0026598418293161848, 1, 0, 0.6659671864819814, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0] <br>\n",
    "(The values in the above example can be different from actual values.)<br>\n",
    "\n",
    "<b>Do not use any library such as sklearn.preprocessing. You can use only Numpy. </b><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9482988418899992, 0, 1, 1, 0, 0, 0, 0.6630430369238169, -0.2869070358990002, 0, 1, 0, 0, 1, 0.06262698816499775, 1, 0, 0.8676874714478687, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0], [1.4164965964633045, 0, 1, 0, 0, 0, 1, 1.4635793093352456, 0.6840435065860354, 1, 0, 0, 0, 1, -1.7211623988794267, 0, 1, 0.2536717495237399, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0], [1.4164965964633045, 0, 1, 0, 0, 0, 1, -0.6711840837618978, -0.36018632212428586, 1, 0, 0, 0, 1, -0.8292677053572144, 0, 1, 1.0979433671694172, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1], [0.9482988418899992, 0, 1, 0, 0, 0, 1, -0.1374932354876119, 0.09780921678374978, 1, 0, 0, 0, 1, -0.06478653948103257, 1, 0, 0.17691978428322372, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1], [0.24600221003004147, 0, 1, 0, 0, 0, 1, 0.396197612786674, -1.0380197197081786, 1, 0, 1, 0, 0, -0.02231536359902246, 1, 0, -0.5905998681219374, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [-0.33924498318659, 0, 1, 0, 0, 1, 0, 2.104008327264389, -0.9097809688139287, 0, 1, 1, 0, 0, 0.572281098749119, 1, 0, -0.5138479028814213, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1], [-0.8074427377598953, 0, 1, 0, 1, 0, 0, -1.2048749320361836, -0.36018632212428586, 1, 0, 1, 0, 0, 0.8271081540411795, 1, 0, -0.1300880766788407, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1], [-0.6903932991165689, 0, 1, 0, 1, 0, 0, -0.1374932354876119, 0.31764707545960685, 1, 0, 1, 0, 0, 0.9545216816872099, 1, 0, -0.4370959376409052, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0.36305164867336776, 1, 0, 1, 0, 0, 0, 0.9298884610609598, 0.6290840419170711, 0, 1, 0, 0, 1, 0.572281098749119, 1, 0, -0.1300880766788407, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0.36305164867336776, 0, 1, 0, 0, 1, 0, -0.03075506583275471, -0.451785429905893, 1, 0, 0, 0, 1, 1.0394640334512302, 1, 0, 1.5584551586125144, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0.36305164867336776, 1, 0, 0, 0, 1, 0, -0.6711840837618978, 1.6733138706273925, 1, 0, 1, 0, 0, 0.99699285756922, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [-1.2756404923332005, 0, 1, 0, 0, 1, 0, -0.1374932354876119, -0.2869070358990002, 1, 0, 1, 0, 0, 1.2942910887432908, 0, 1, -0.5905998681219374, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0.24600221003004147, 0, 1, 0, 0, 0, 1, 0.9298884610609598, 0.5008452910228212, 1, 0, 0, 0, 1, -1.5512776953513863, 0, 1, -0.4370959376409052, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0], [1.182397719176652, 1, 0, 0, 0, 0, 1, 0.9298884610609598, -0.4334656083495716, 1, 0, 0, 0, 1, -1.466335343587366, 1, 0, -0.1300880766788407, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1], [1.8846943510366096, 1, 0, 0, 1, 0, 0, 1.4635793093352456, 0.9771606514871782, 1, 0, 1, 0, 0, 0.572281098749119, 1, 0, -0.5905998681219374, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0], [0.36305164867336776, 0, 1, 0, 0, 1, 0, -1.0981367623813265, -0.34186650056796447, 1, 0, 0, 0, 1, 0.6996946263951493, 1, 0, 1.021191401928901, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1], [-0.5733438604732426, 0, 1, 0, 0, 0, 1, 0.9298884610609598, -0.1037088203357859, 1, 0, 0, 0, 1, -0.8717388812392245, 1, 0, 1.0979433671694172, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [1.182397719176652, 0, 1, 0, 0, 0, 1, -0.6711840837618978, -1.312817043053, 1, 0, 1, 0, 0, -0.3620847706551033, 1, 0, -0.5905998681219374, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1], [-1.2756404923332005, 0, 1, 0, 0, 0, 1, -1.0981367623813265, 0.7573227928113211, 1, 0, 0, 0, 1, 0.19004051581102804, 1, 0, -0.8976077290840019, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0], [-0.10514610589993742, 0, 1, 0, 0, 1, 0, -0.4043386596247548, 0.4458858263538569, 1, 0, 0, 0, 1, 0.14756933992901794, 1, 0, -0.5138479028814213, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0], [-0.4562944218299163, 1, 0, 0, 0, 0, 1, -0.1374932354876119, 1.0321201161561424, 1, 0, 1, 0, 0, -0.2771424188910831, 0, 1, 0.0234158538021915, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [-0.10514610589993742, 0, 1, 0, 0, 0, 1, -0.6711840837618978, -1.1112990059334644, 1, 0, 1, 0, 0, -1.508806519469376, 1, 0, 0.17691978428322372, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1], [0.5971505259600204, 0, 1, 0, 0, 0, 1, 0.6630430369238169, 0.6107642203607497, 1, 0, 0, 0, 1, -0.2771424188910831, 0, 1, 1.2514472976504494, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1], [0.5971505259600204, 0, 1, 0, 0, 1, 0, 0.396197612786674, -1.1662584706024286, 1, 0, 0, 0, 1, 0.27498286757504825, 1, 0, 1.4049512281314818, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [0.4801010873166941, 0, 1, 0, 0, 0, 1, 1.9972701576095315, 1.4168363688388925, 1, 0, 0, 0, 1, -0.3620847706551033, 0, 1, 1.7119590890935459, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1], [1.182397719176652, 0, 1, 0, 0, 0, 1, -1.2048749320361836, -0.012109712554178781, 1, 0, 0, 0, 1, 0.40239639522107856, 1, 0, -0.4370959376409052, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0], [-1.2756404923332005, 0, 1, 0, 0, 0, 1, -1.2048749320361836, -0.9464206119265715, 1, 0, 0, 0, 1, 1.2093487369792706, 1, 0, -0.8976077290840019, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0], [0.5971505259600204, 0, 1, 0, 0, 0, 1, -0.4043386596247548, 0.17108850300903547, 1, 0, 0, 0, 1, -0.3196135947730932, 0, 1, 1.2514472976504494, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1], [-0.4562944218299163, 1, 0, 0, 0, 1, 0, 0.396197612786674, 1.0870795808251068, 1, 0, 0, 0, 1, -0.2771424188910831, 1, 0, 0.2536717495237399, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0], [0.36305164867336776, 0, 1, 0, 0, 0, 1, 0.9298884610609598, 0.3909263616848926, 1, 0, 0, 0, 1, -1.5937488712333963, 0, 1, -0.2835920071598729, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1], [-1.158591053689874, 0, 1, 0, 0, 0, 1, -1.5250894410007552, -0.7449025748070358, 1, 0, 0, 0, 1, -0.02231536359902246, 0, 1, 1.4049512281314818, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [-1.860887685549832, 0, 1, 0, 0, 1, 0, 0.396197612786674, 1.3252372610572853, 1, 0, 0, 0, 1, 1.421704616389321, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [-0.22219554454326373, 1, 0, 0, 0, 0, 1, 0.2894594431318168, -0.26858721434267874, 1, 0, 0, 0, 1, 0.48733874698509877, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [-0.4562944218299163, 1, 0, 0, 0, 1, 0, -0.1374932354876119, 0.13444885989639263, 1, 0, 0, 0, 1, 0.020155812282987642, 1, 0, -0.5138479028814213, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [1.2994471578199782, 0, 1, 0, 0, 0, 1, -0.6711840837618978, 0.9771606514871782, 1, 0, 0, 0, 1, 0.10509816404700785, 1, 0, -0.5905998681219374, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [0.831249403246673, 1, 0, 0, 0, 0, 1, 1.4635793093352456, -1.5509747232851785, 1, 0, 0, 0, 1, -0.14972889124505276, 1, 0, 3.861014115827998, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [-0.33924498318659, 0, 1, 0, 1, 0, 0, 0.07598310382210247, -0.8731413257012858, 1, 0, 1, 0, 0, 0.40239639522107856, 1, 0, -0.2835920071598729, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0], [-0.8074427377598953, 0, 1, 0, 0, 0, 1, -0.5644459141070406, -0.4884250730185358, 1, 0, 0, 0, 1, 1.5915893199173614, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0.24600221003004147, 1, 0, 0, 0, 0, 1, -0.24423140514246905, 0.9954804730434996, 1, 0, 0, 0, 1, 0.44486757110308867, 1, 0, -0.8976077290840019, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0], [0.4801010873166941, 0, 1, 0, 0, 0, 1, 0.396197612786674, -1.312817043053, 1, 0, 1, 0, 0, 0.572281098749119, 0, 1, -0.8976077290840019, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1], [0.24600221003004147, 0, 1, 0, 0, 1, 0, -0.24423140514246905, -0.36018632212428586, 1, 0, 0, 0, 1, 0.06262698816499775, 1, 0, -0.5905998681219374, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1], [-1.860887685549832, 0, 1, 0, 0, 0, 1, -0.7779222534167549, -0.5433845376875002, 1, 0, 1, 0, 0, -0.3620847706551033, 1, 0, 0.0234158538021915, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [0.7141999646033467, 1, 0, 0, 0, 0, 1, 0.6630430369238169, 1.0687597592687854, 1, 0, 0, 0, 1, -0.10725771536304267, 0, 1, -0.1300880766788407, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [0.12895277138671518, 0, 1, 0, 0, 0, 1, -0.4043386596247548, 0.006210109002142645, 0, 1, 0, 0, 1, -0.19220006712706286, 0, 1, 0.0234158538021915, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0], [0.36305164867336776, 0, 1, 0, 0, 1, 0, 0.396197612786674, -0.6899431101380715, 0, 1, 0, 0, 1, 0.6996946263951493, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [-2.329085440123137, 1, 0, 0, 0, 0, 1, 0.2894594431318168, -1.2028981137150714, 1, 0, 1, 0, 0, 1.421704616389321, 1, 0, 0.17691978428322372, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0.9482988418899992, 1, 0, 0, 0, 0, 1, 0.9298884610609598, 2.900741914900928, 1, 0, 0, 0, 1, 0.23251169169303815, 1, 0, 2.1724708805366433, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1], [0.011903332743388876, 0, 1, 0, 0, 0, 1, 0.396197612786674, -0.580024180800143, 1, 0, 1, 0, 0, -1.5937488712333963, 0, 1, 3.4005023243849006, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1], [1.182397719176652, 0, 1, 1, 0, 0, 0, 0.2894594431318168, 0.6107642203607497, 0, 1, 0, 0, 1, 1.0819352093332402, 1, 0, 0.17691978428322372, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0], [-1.158591053689874, 1, 0, 0, 1, 0, 0, -0.1374932354876119, -0.26858721434267874, 1, 0, 0, 0, 1, 1.1244063852152504, 1, 0, -0.4370959376409052, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [0.12895277138671518, 1, 0, 0, 0, 0, 1, 3.5983427024323893, 0.7206831496986782, 0, 1, 0, 0, 1, -0.659383001829174, 0, 1, 2.1724708805366433, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [-0.10514610589993742, 0, 1, 0, 0, 0, 1, -1.2048749320361836, -0.1769881065610716, 1, 0, 1, 0, 0, -0.9566812330032447, 0, 1, 1.2514472976504494, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1], [-1.2756404923332005, 0, 1, 0, 1, 0, 0, -0.6711840837618978, -0.5250647161311787, 1, 0, 1, 0, 0, 0.9120505058051998, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0.831249403246673, 1, 0, 0, 0, 0, 1, -0.4577077444521834, -0.7265827532507144, 1, 0, 1, 0, 0, 0.6147522746311291, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [-0.10514610589993742, 0, 1, 0, 0, 1, 0, -0.6711840837618978, 0.17108850300903547, 1, 0, 0, 0, 1, -0.06478653948103257, 1, 0, -0.5905998681219374, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [-0.4562944218299163, 0, 1, 0, 0, 0, 1, 0.396197612786674, 0.22604796767799976, 1, 0, 0, 0, 1, 1.5915893199173614, 0, 1, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [-1.3926899309765268, 1, 0, 0, 0, 1, 0, -0.5644459141070406, -0.6533034670254286, 1, 0, 1, 0, 0, 0.6996946263951493, 1, 0, -0.7441037986029697, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [0.011903332743388876, 1, 0, 0, 1, 0, 0, 0.12935218864953105, 0.02452993055846407, 1, 0, 0, 0, 1, 0.5298099228671089, 1, 0, 0.17691978428322372, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [1.7676449123932834, 0, 1, 0, 0, 0, 1, 0.6630430369238169, -1.3677765077219644, 1, 0, 1, 0, 0, -0.9991524088852548, 0, 1, 1.0979433671694172, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1], [0.831249403246673, 0, 1, 0, 1, 0, 0, -0.6711840837618978, 0.5924443988044282, 1, 0, 0, 0, 1, -1.933518278289477, 1, 0, 0.17691978428322372, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1], [-2.329085440123137, 0, 1, 0, 0, 0, 1, -0.6711840837618978, -0.9281007903702501, 1, 0, 1, 0, 0, -0.7867965294752043, 0, 1, 0.3304237147642561, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [0.4801010873166941, 0, 1, 1, 0, 0, 0, 1.9972701576095315, 0.7206831496986782, 1, 0, 0, 0, 1, 0.44486757110308867, 1, 0, -0.7441037986029697, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [-0.33924498318659, 0, 1, 0, 1, 0, 0, -0.24423140514246905, -0.7998620394760001, 0, 1, 1, 0, 0, 1.5066469681533412, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [1.0653482805333256, 0, 1, 0, 0, 1, 0, -0.4043386596247548, 1.1053994023814282, 1, 0, 1, 0, 0, -0.7443253535931942, 0, 1, 0.4839276452452883, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [0.36305164867336776, 0, 1, 0, 0, 1, 0, -1.4717203561733265, -0.1586682850047502, 1, 0, 0, 0, 1, 0.23251169169303815, 0, 1, -0.4370959376409052, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [-1.158591053689874, 0, 1, 0, 1, 0, 0, -0.24423140514246905, 1.0870795808251068, 1, 0, 0, 0, 1, 0.9120505058051998, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [1.4164965964633045, 1, 0, 0, 0, 1, 0, -0.9380295078990407, 5.7769538992433915, 1, 0, 0, 0, 1, 0.48733874698509877, 1, 0, 0.3304237147642561, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [1.0653482805333256, 0, 1, 0, 0, 0, 1, -0.6711840837618978, -0.048749355666821634, 1, 0, 0, 0, 1, -2.2308165094635477, 0, 1, 0.7909355062073529, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0], [-0.4562944218299163, 0, 1, 0, 0, 0, 1, 0.396197612786674, 0.922201186818214, 1, 0, 1, 0, 0, 1.0394640334512302, 0, 1, 0.3304237147642561, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1], [-0.10514610589993742, 1, 0, 0, 0, 1, 0, -1.2048749320361836, -0.6349836454691072, 1, 0, 1, 0, 0, 0.40239639522107856, 1, 0, 0.3304237147642561, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [-0.8074427377598953, 0, 1, 0, 0, 1, 0, -0.4577077444521834, 0.1161290383400712, 0, 1, 1, 0, 0, 1.1244063852152504, 1, 0, -0.8976077290840019, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0], [-1.158591053689874, 1, 0, 0, 1, 0, 0, -1.0981367623813265, -1.6242540095104643, 1, 0, 1, 0, 0, -0.4470271224191235, 1, 0, -0.8976077290840019, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [1.7676449123932834, 0, 1, 0, 0, 1, 0, 1.4635793093352456, 0.37260654012857114, 1, 0, 1, 0, 0, -1.5512776953513863, 0, 1, 1.3281992628909656, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1], [-0.22219554454326373, 0, 1, 0, 0, 0, 1, 0.5029357824415311, -0.41514578679325015, 1, 0, 0, 0, 1, -1.5937488712333963, 0, 1, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1], [-0.33924498318659, 0, 1, 0, 0, 0, 1, -1.3116131016910408, -0.2869070358990002, 0, 1, 1, 0, 0, -0.06478653948103257, 1, 0, -0.8208557638434857, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1], [0.12895277138671518, 0, 1, 0, 0, 0, 1, -0.03075506583275471, -1.18457829215875, 1, 0, 0, 0, 1, -1.8485759265254569, 0, 1, 0.7141835409668367, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0], [-1.509739369619853, 0, 1, 1, 0, 0, 0, 0.8231502914061026, -0.08538899877946449, 1, 0, 0, 0, 1, 1.2518199128612806, 1, 0, -0.2835920071598729, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0], [0.4801010873166941, 0, 1, 1, 0, 0, 0, 2.42422283622896, 0.3909263616848926, 1, 0, 0, 0, 1, -0.14972889124505276, 1, 0, 2.3259748110176757, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1], [0.5971505259600204, 1, 0, 0, 0, 0, 1, 1.3568411396803886, 1.0321201161561424, 1, 0, 0, 0, 1, 0.5298099228671089, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [-1.509739369619853, 0, 1, 0, 0, 1, 0, -0.6711840837618978, -0.1586682850047502, 0, 1, 1, 0, 0, 1.9313587269734422, 1, 0, -0.2835920071598729, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1], [1.2994471578199782, 0, 1, 0, 1, 0, 0, 1.4635793093352456, -0.048749355666821634, 1, 0, 1, 0, 0, -1.2115082882953054, 0, 1, -0.8976077290840019, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0], [-0.10514610589993742, 0, 1, 0, 1, 0, 0, 3.1713900238129606, 0.6290840419170711, 1, 0, 0, 0, 1, 1.9738299028554525, 1, 0, -0.8976077290840019, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1], [0.831249403246673, 1, 0, 0, 0, 0, 1, 0.2894594431318168, 0.8306020790366068, 0, 1, 1, 0, 0, -1.8061047506434469, 1, 0, 0.5606796104858043, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0], [1.4164965964633045, 0, 1, 0, 0, 0, 1, -1.7385657803104695, 0.922201186818214, 1, 0, 0, 0, 1, -0.9991524088852548, 0, 1, -0.20684004191935682, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0], [1.650595473749957, 0, 1, 1, 0, 0, 0, 1.4635793093352456, -0.26858721434267874, 0, 1, 0, 0, 1, -0.7443253535931942, 1, 0, -0.8208557638434857, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0], [-1.158591053689874, 1, 0, 0, 0, 0, 1, 0.2894594431318168, -0.2319475712300359, 1, 0, 0, 0, 1, 0.14756933992901794, 0, 1, -0.7441037986029697, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [-0.5733438604732426, 1, 0, 0, 0, 0, 1, -1.2048749320361836, 0.09780921678374978, 1, 0, 0, 0, 1, 0.44486757110308867, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0.24600221003004147, 0, 1, 0, 0, 1, 0, 0.9298884610609598, -2.2471279424253927, 0, 1, 1, 0, 0, 1.0394640334512302, 1, 0, -0.7441037986029697, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1], [0.36305164867336776, 0, 1, 0, 0, 0, 1, -0.24423140514246905, 0.1894083245653569, 1, 0, 0, 0, 1, -0.7867965294752043, 0, 1, 1.4049512281314818, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1], [0.011903332743388876, 0, 1, 0, 1, 0, 0, -0.1374932354876119, 0.24436778923432118, 1, 0, 1, 0, 0, 0.27498286757504825, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [-1.0415416150465477, 1, 0, 0, 1, 0, 0, -1.4717203561733265, -0.8181818610323215, 1, 0, 1, 0, 0, 0.99699285756922, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [1.0653482805333256, 1, 0, 0, 0, 0, 1, -0.1374932354876119, 0.9954804730434996, 1, 0, 1, 0, 0, -1.1265659365312852, 1, 0, 0.6374315757263205, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0], [-1.860887685549832, 1, 0, 0, 0, 1, 0, -2.058780289275041, -0.9097809688139287, 1, 0, 1, 0, 0, 1.2942910887432908, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0.9482988418899992, 1, 0, 0, 0, 0, 1, -1.3116131016910408, 0.37260654012857114, 1, 0, 1, 0, 0, 0.8695793299231896, 0, 1, 0.4839276452452883, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0], [-0.33924498318659, 1, 0, 0, 0, 1, 0, 0.18272127347695963, -0.964740433482893, 1, 0, 0, 0, 1, 0.8695793299231896, 1, 0, -0.8208557638434857, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [-0.6903932991165689, 0, 1, 0, 0, 1, 0, -0.7779222534167549, -1.82577204663, 1, 0, 0, 0, 1, -0.9566812330032447, 1, 0, -0.2835920071598729, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0], [-0.10514610589993742, 0, 1, 0, 0, 0, 1, -0.5644459141070406, 0.6840435065860354, 1, 0, 0, 0, 1, -1.3813929918233459, 0, 1, 1.5584551586125144, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0], [0.12895277138671518, 0, 1, 0, 0, 0, 1, -0.1374932354876119, 0.6290840419170711, 0, 1, 0, 0, 1, -1.933518278289477, 0, 1, 0.3304237147642561, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1], [-1.0415416150465477, 0, 1, 0, 0, 0, 1, -0.6711840837618978, 0.006210109002142645, 1, 0, 0, 0, 1, -0.19220006712706286, 1, 0, -0.2835920071598729, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1], [-0.6903932991165689, 1, 0, 0, 1, 0, 0, 0.07598310382210247, 0.40924618324121403, 1, 0, 1, 0, 0, 0.572281098749119, 1, 0, -0.8976077290840019, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [-1.509739369619853, 0, 1, 0, 1, 0, 0, -0.6711840837618978, 0.8489219005929283, 1, 0, 1, 0, 0, 0.572281098749119, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [-1.6267888082631794, 1, 0, 0, 1, 0, 0, -0.3509695747973262, 1.0504399377124638, 1, 0, 1, 0, 0, 0.6147522746311291, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [1.4164965964633045, 0, 1, 0, 0, 0, 1, -0.6711840837618978, -0.21362774967371445, 1, 0, 1, 0, 0, -3.2925959065138004, 1, 0, -0.1300880766788407, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [0.36305164867336776, 0, 1, 0, 0, 0, 1, -1.7385657803104695, -0.26858721434267874, 1, 0, 1, 0, 0, 0.31745404345705835, 1, 0, -0.8208557638434857, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1], [-1.509739369619853, 1, 0, 0, 0, 1, 0, -0.6711840837618978, -0.7265827532507144, 1, 0, 1, 0, 0, 1.0394640334512302, 1, 0, -0.8976077290840019, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [1.7676449123932834, 0, 1, 0, 1, 0, 0, 1.2501029700255313, -0.06706917722314305, 1, 0, 0, 0, 1, -0.23467124300907297, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [-1.2756404923332005, 1, 0, 0, 0, 1, 0, -0.7779222534167549, -0.12202864189210734, 1, 0, 1, 0, 0, 0.020155812282987642, 1, 0, -0.6673518333624535, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0], [-1.2756404923332005, 0, 1, 0, 0, 1, 0, -0.6711840837618978, -0.41514578679325015, 1, 0, 1, 0, 0, 0.8695793299231896, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0.7141999646033467, 0, 1, 0, 0, 0, 1, 0.2894594431318168, -1.5143350801725357, 1, 0, 0, 0, 1, -0.9991524088852548, 0, 1, 1.8654630195745783, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0], [-1.509739369619853, 0, 1, 0, 0, 0, 1, 0.18272127347695963, 1.2153183317193568, 1, 0, 1, 0, 0, -0.9991524088852548, 0, 1, 0.4839276452452883, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [0.7141999646033467, 0, 1, 0, 0, 0, 1, 0.396197612786674, -0.7632223963633572, 1, 0, 0, 0, 1, -0.4470271224191235, 0, 1, 0.5606796104858043, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1], [1.2994471578199782, 0, 1, 0, 0, 0, 1, 1.4635793093352456, -0.3785061436806073, 1, 0, 0, 0, 1, -0.4470271224191235, 1, 0, 0.8676874714478687, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0], [0.4801010873166941, 0, 1, 1, 0, 0, 0, 0.07598310382210247, -0.8181818610323215, 1, 0, 1, 0, 0, 0.572281098749119, 1, 0, -0.2835920071598729, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0], [1.2994471578199782, 1, 0, 0, 0, 1, 0, 0.7164121217512455, 0.5374849341354639, 1, 0, 0, 0, 1, 0.14756933992901794, 1, 0, -0.8976077290840019, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0], [0.011903332743388876, 1, 0, 0, 0, 0, 1, -0.24423140514246905, -0.7998620394760001, 1, 0, 0, 1, 0, -0.7867965294752043, 0, 1, 0.6374315757263205, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1], [0.36305164867336776, 0, 1, 0, 0, 0, 1, -0.9913985927264692, 1.270277796388321, 1, 0, 0, 1, 0, -0.3620847706551033, 1, 0, 2.479478741498708, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0], [-1.6267888082631794, 0, 1, 0, 1, 0, 0, -0.6711840837618978, -1.6792134741794285, 1, 0, 1, 0, 0, 1.421704616389321, 1, 0, -0.8976077290840019, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0.24600221003004147, 1, 0, 0, 0, 0, 1, 0.396197612786674, -0.14034846344842877, 1, 0, 1, 0, 0, -1.084094760649275, 0, 1, -0.7441037986029697, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from collections import Iterable\n",
    "\n",
    "def flatten(items):\n",
    "    \"\"\"Yield items from any nested iterable; see Reference.\"\"\"\n",
    "    for x in items:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            for sub_x in flatten(x):\n",
    "                yield sub_x\n",
    "        else:\n",
    "            yield x\n",
    "\n",
    "            \n",
    "def one_hot(training_list) :\n",
    "    return_list = []\n",
    "    category_dict = {i : c for c, i in enumerate(set(training_list))}\n",
    "    \n",
    "    for x in training_list:\n",
    "        arr = np.zeros(len(category_dict), dtype=int)\n",
    "        np.put(arr, category_dict[x], 1)\n",
    "        return_list.append(arr.tolist())\n",
    "\n",
    "    return return_list\n",
    "\n",
    "\n",
    "\n",
    "numerical = [0, 3, 4, 7, 9]\n",
    "categorical = list(set([i for i in range(13)]) - set(numerical))\n",
    "\n",
    "# numerical\n",
    "for i in numerical:\n",
    "    X_tr_list_ith = [row[i] for row in X_tr]\n",
    "    \n",
    "    mean = sum(X_tr_list_ith)/len(X_tr_list_ith)\n",
    "    std = np.std(X_tr_list_ith)\n",
    "    \n",
    "    #normalization\n",
    "    for row in X_tr :\n",
    "        row[i] = (row[i] - mean)/std\n",
    "    \n",
    "# categorical\n",
    "for i in categorical:\n",
    "    new_list = []\n",
    "    X_tr_list_ith = [int(row[i]) for row in X_tr]\n",
    "    new_list = one_hot(X_tr_list_ith)\n",
    "    for index, row in enumerate(X_tr):\n",
    "        row[i] = new_list[index]\n",
    "\n",
    "        \n",
    "# get pre-processed train data\n",
    "tmp = []\n",
    "for row in X_tr:\n",
    "    tmp.append(list(flatten(row)))\n",
    "X_tr = []\n",
    "X_tr = tmp\n",
    "print(X_tr)\n",
    "# End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Logistic regression model and regularizer\n",
    "Build logistic regression model with l2 regularization utilizing sklearn. <br>\n",
    "Find the optimal coefficient based on <b>cross entropy loss</b> on thet validation set. <br>\n",
    "Try following regularization parameter settings.\n",
    "* Regularization parameters = 0.01, 0.05, 0.1, 0.5, 1, 10, 100 <br>\n",
    "* Note that regluarization parameter for LogisticRegression in sklearn is inverse of true parameter. <br>\n",
    "  (coef = 0.001 for LogisticRegression   =>  $\\lambda$ = 1000 in our course note)\n",
    "* Your model should be <b>LogisticRegression(C=coef, solver='lbfgs', max_iter=500). </b>\n",
    "  <br>  <b>Do not change the model setting except C. </b> \n",
    "  <br> (coef = each regularization parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please use below function\n",
    "# logreg = LogisticRegression(C=coef, solver='lbfgs', max_iter=500)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "coefs = [0.01, 0.05, 0.1, 0.5, 1, 10, 100]\n",
    "\n",
    "opt_coef = 1\n",
    "\n",
    "\n",
    "# To plot losses on training and validation sets with varied parameter settings, \n",
    "# save them on lists.\n",
    "loss_tr, loss_va = [],[]\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# End your code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Plot error\n",
    "Plot the train and validation loss against given regularization parameter <b>(not inverse)</b>.<br>\n",
    "<b> Analyze the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not fix the code!!\n",
    "\n",
    "plt.plot(coefs, loss_tr, coefs, loss_va, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze \n",
    "Write explanation of graph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Model selection and evaluation\n",
    "\n",
    "Drop the performance on test set with the regularization coefficient of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code here\n",
    "\n",
    "# End your code\n",
    "\n",
    "\n",
    "#print regularization paramter of final model and drop test loss and accuracy\n",
    "print (\"Optimal : {}, Loss : {:2.3f}, Accuracy : {:3.2f}\".format(coef, test_loss, test_acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
